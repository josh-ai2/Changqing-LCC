{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T14:12:58.467156Z",
     "start_time": "2025-02-17T14:12:57.459080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#csv summarizer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to analyze CSV file\n",
    "def analyze_csv(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Get column names and data types\n",
    "    column_info = df.dtypes.reset_index()\n",
    "    column_info.columns = [\"Column Name\", \"Data Type\"]\n",
    "\n",
    "    # Get dataset size\n",
    "    dataset_size = df.shape\n",
    "\n",
    "    # Display results\n",
    "    print(\"Column Names and Data Types:\")\n",
    "    print(column_info.to_string(index=False))\n",
    "    print(\"\\nTotal Rows:\", dataset_size[0])\n",
    "    print(\"Total Columns:\", dataset_size[1])\n",
    "\n",
    "# Example usage\n",
    "file_path = \"Dataset/raw_cleaned/LarchCN_256x256_2022_monthly_patch_0_0_cleaned.csv\"  # Change to your CSV file path\n",
    "analyze_csv(file_path)\n"
   ],
   "id": "37c6e0c2dddedb1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names and Data Types:\n",
      "Column Name Data Type\n",
      "        AOT     int64\n",
      "         B1     int64\n",
      "        B11     int64\n",
      "        B12     int64\n",
      "         B2     int64\n",
      "         B3     int64\n",
      "         B4     int64\n",
      "         B5     int64\n",
      "         B6     int64\n",
      "         B7     int64\n",
      "         B8     int64\n",
      "        B8A     int64\n",
      "         B9     int64\n",
      "        EVI   float64\n",
      "       NDVI   float64\n",
      "       NDWI   float64\n",
      "      TCI_B     int64\n",
      "      TCI_G     int64\n",
      "      TCI_R     int64\n",
      "        WVP     int64\n",
      " image_date    object\n",
      "  longitude   float64\n",
      "   latitude   float64\n",
      "\n",
      "Total Rows: 788480\n",
      "Total Columns: 23\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T14:01:42.929073Z",
     "start_time": "2025-02-17T14:01:42.869777Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('Dataset/groundtruth_raw/Original Groundtruth/CQ_HBY_GroundTruth_Cleaned0213_Cat7.csv')\n",
    "\n",
    "# Remove the 'reference' column if it exists\n",
    "if 'reference' in df.columns:\n",
    "    df = df.drop(columns=['ref'])\n",
    "\n",
    "# Rename columns:\n",
    "# 'forest' -> 'Class', 'lon' -> 'longitude', 'lat' -> 'latitude'\n",
    "rename_map = {\n",
    "    'forest': 'Class',\n",
    "    'lon': 'longitude',\n",
    "    'lat': 'latitude'\n",
    "}\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Ensure 'Class' is a string\n",
    "if 'Class' in df.columns:\n",
    "    df['Class'] = df['Class'].astype(str)\n",
    "\n",
    "# Convert 'latitude' and 'longitude' columns to numeric (if they exist)\n",
    "if 'latitude' in df.columns:\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "if 'longitude' in df.columns:\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "# Save the cleaned CSV to a new file\n",
    "df.to_csv('cleaned.csv', index=False)\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T14:02:21.800627Z",
     "start_time": "2025-02-17T14:02:21.790026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV file\n",
    "df = pd.read_csv('Dataset/groundtruth_raw/Original Groundtruth/0213-blind-cleaned.csv')\n",
    "\n",
    "# Calculate the counts of each unique class in the 'Class' column\n",
    "class_counts = df['Class'].value_counts()\n",
    "\n",
    "# Print the unique class types along with the number of data points (rows) for each class\n",
    "print(\"Unique Class Types and their counts:\")\n",
    "print(class_counts)\n"
   ],
   "id": "c92ef638db751e99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Class Types and their counts:\n",
      "Class\n",
      "larch_JP          1340\n",
      "deci_broad        1212\n",
      "ever_coni          819\n",
      "mix_coni_broad     789\n",
      "larch_CN           229\n",
      "shrubland          199\n",
      "ever_broad          39\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "GEE code for larch_CN\n",
    "/***************************************\n",
    " * GEE Script: Sample a 2×2 patch around\n",
    " * \"larch_CN\" ground‐truth points\n",
    " * with a small buffer + earliest monthly\n",
    " * S2 images, exporting to a single CSV.\n",
    " ***************************************/\n",
    "\n",
    "// -------------------- 1. Load Ground-Truth Asset --------------------\n",
    "// Update this to your correct asset ID:\n",
    "var groundTruthAsset = \"projects/ee-my-josh-ai/assets/cleaned\";\n",
    "// Example columns: [\"Class\",\"longitude\",\"latitude\"]\n",
    "var allPoints = ee.FeatureCollection(groundTruthAsset);\n",
    "print(\"DEBUG: # of allPoints:\", allPoints.size());\n",
    "\n",
    "// -------------------- 2. Filter to larch_CN Only --------------------\n",
    "var larchCN = allPoints.filter(ee.Filter.eq(\"Class\", \"larch_CN\"));\n",
    "print(\"DEBUG: # of larch_CN points:\", larchCN.size());\n",
    "\n",
    "// If you want to see them on the map, do:\n",
    "Map.addLayer(larchCN, {color: \"red\"}, \"larch_CN Points\");\n",
    "\n",
    "// -------------------- 3. Buffer Each larch_CN Point (2×2 patch) --------------------\n",
    "// We'll buffer by 10 m, then take the .bounds(...) to get a small rectangle ~20×20 m.\n",
    "// Use maxError in both operations to avoid geometry errors.\n",
    "var bufferDist = 10; // 10 m\n",
    "var larchBuffered = larchCN.map(function(feat) {\n",
    "  var geom = feat.geometry();\n",
    "  var buffered = geom.buffer({distance: bufferDist, maxError: 1})\n",
    "                     .bounds({maxError: 1});\n",
    "  return feat.setGeometry(buffered);\n",
    "});\n",
    "Map.addLayer(larchBuffered, {color: \"blue\"}, \"Buffered larch_CN patches\");\n",
    "\n",
    "// -------------------- 4. Define Date Range & S2 Collection --------------------\n",
    "var startDate = \"2022-01-01\";\n",
    "var endDate   = \"2022-12-31\";\n",
    "\n",
    "var s2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "            .filterDate(startDate, endDate)\n",
    "            .filterBounds(larchBuffered.geometry())\n",
    "            .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 20));\n",
    "\n",
    "// (A) Optional cloud mask:\n",
    "function maskClouds(image) {\n",
    "  var qa = image.select(\"QA60\");\n",
    "  var cloudBitMask = 1 << 10;\n",
    "  var cirrusBitMask = 1 << 11;\n",
    "  var mask = qa.bitwiseAnd(cloudBitMask).eq(0)\n",
    "              .and(qa.bitwiseAnd(cirrusBitMask).eq(0));\n",
    "  return image.updateMask(mask);\n",
    "}\n",
    "\n",
    "// (B) Compute NDVI, EVI, NDWI:\n",
    "function addIndices(image) {\n",
    "  var ndvi = image.normalizedDifference([\"B8\", \"B4\"]).rename(\"NDVI\");\n",
    "  var evi = image.expression(\n",
    "    \"2.5*((NIR-RED)/(NIR+6*RED-7.5*BLUE+1))\", {\n",
    "      \"NIR\": image.select(\"B8\"),\n",
    "      \"RED\": image.select(\"B4\"),\n",
    "      \"BLUE\": image.select(\"B2\")\n",
    "    }\n",
    "  ).rename(\"EVI\");\n",
    "  var ndwi = image.expression(\n",
    "    \"(GREEN - NIR)/(GREEN + NIR)\", {\n",
    "      \"GREEN\": image.select(\"B3\"),\n",
    "      \"NIR\": image.select(\"B8\")\n",
    "    }\n",
    "  ).rename(\"NDWI\");\n",
    "  return image.addBands([ndvi, evi, ndwi]);\n",
    "}\n",
    "\n",
    "// (C) Select final bands:\n",
    "function selectBands(image) {\n",
    "  var originalBands = [\n",
    "    \"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\n",
    "    \"B9\",\"B11\",\"B12\",\"AOT\",\"WVP\"\n",
    "  ];\n",
    "  var rgbBands = [\"TCI_R\",\"TCI_G\",\"TCI_B\"];\n",
    "  var indexBands = [\"NDVI\",\"EVI\",\"NDWI\"];\n",
    "  return image.select(originalBands.concat(rgbBands).concat(indexBands));\n",
    "}\n",
    "\n",
    "var s2Processed = s2\n",
    "  .map(maskClouds)\n",
    "  .map(addIndices)\n",
    "  .map(selectBands)\n",
    "  // unmask to fill missing with -999\n",
    "  .map(function(img) { return img.unmask(-999); });\n",
    "\n",
    "// -------------------- 5. Sample Earliest Monthly Image for Each Point --------------------\n",
    "var months = ee.List.sequence(1, 12);\n",
    "\n",
    "// We'll build a single FeatureCollection containing all points × months:\n",
    "var allSamples = months.map(function(m) {\n",
    "  // For each month, we pick the earliest image & sample all larch_CN patches:\n",
    "  var monthlyImages = s2Processed.filter(ee.Filter.calendarRange(m, m, \"month\"))\n",
    "                                 .sort(\"system:time_start\");\n",
    "  var firstImage = ee.Image(monthlyImages.first());\n",
    "\n",
    "  // If no image found, return an empty FeatureCollection:\n",
    "  var hasImage = monthlyImages.size().gt(0);\n",
    "  var sampled = ee.FeatureCollection(ee.Algorithms.If(\n",
    "    hasImage,\n",
    "    // If an image exists, sample each geometry in larchBuffered:\n",
    "    firstImage.sampleRegions({\n",
    "      collection: larchBuffered,    // each geometry = small 2×2 patch\n",
    "      scale: 10,\n",
    "      projection: \"EPSG:4326\",\n",
    "      geometries: true // preserve lat/lon geometry\n",
    "    }).map(function(feat) {\n",
    "      // Attach the image_date & the month, etc.\n",
    "      return feat.set({\n",
    "        \"image_date\": firstImage.date().format(\"YYYY-MM-dd\"),\n",
    "        \"month\": m\n",
    "      });\n",
    "    }),\n",
    "    // else no image => empty\n",
    "    ee.FeatureCollection([])\n",
    "  ));\n",
    "\n",
    "  return sampled;\n",
    "});\n",
    "\n",
    "// Flatten into one FeatureCollection:\n",
    "var combinedSamples = ee.FeatureCollection(allSamples).flatten();\n",
    "print(\"DEBUG: Combined samples count:\", combinedSamples.size());\n",
    "\n",
    "// -------------------- 6. Export to Drive --------------------\n",
    "// Single CSV for all \"larch_CN\" points across 12 months:\n",
    "Export.table.toDrive({\n",
    "  collection: combinedSamples,\n",
    "  description: \"larchCN_smallPatch_timeSeries\",\n",
    "  fileFormat: \"CSV\",\n",
    "  folder: \"my_groundtruth_exports\"\n",
    "});\n",
    "\n",
    "print(\"Script finished. Check Tasks tab for your export.\");\n"
   ],
   "id": "d3ec2fb9f85ac70d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaning the time series data, specifically the .geo loc\n",
   "id": "ba6a235ee6e61184"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T11:45:49.710939Z",
     "start_time": "2025-02-17T11:45:46.190559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# DO NOT MODIFY THE FOLLOWING clean_csv FUNCTION (including its comments)\n",
    "# -------------------------------------------------------------------\n",
    "def clean_csv(input_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Extract latitude and longitude from .geo column using regex\n",
    "    def extract_coordinates(geo_str):\n",
    "        match = re.search(r'\\[([-+]?\\d*\\.\\d+),\\s*([-+]?\\d*\\.\\d+)\\]', geo_str)\n",
    "        if match:\n",
    "            return float(match.group(1)), float(match.group(2))\n",
    "        return None, None\n",
    "\n",
    "    df['longitude'], df['latitude'] = zip(*df['.geo'].apply(extract_coordinates))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['system:index', 'month', '.geo'])\n",
    "\n",
    "    # Ensure all feature columns are numeric\n",
    "    feature_cols = [\n",
    "        \"AOT\", \"B1\", \"B11\", \"B12\", \"B2\", \"B3\", \"B4\", \"B5\",\n",
    "        \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"EVI\", \"NDVI\", \"NDWI\",\n",
    "        \"TCI_B\", \"TCI_G\", \"TCI_R\", \"WVP\"\n",
    "    ]\n",
    "    for col in feature_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Handle missing values (forward fill)\n",
    "    df[feature_cols] = df[feature_cols].ffill()\n",
    "\n",
    "    # Save the cleaned CSV\n",
    "    output_path = os.path.splitext(input_path)[0] + \"_cleaned.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned CSV saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Function to Detect Non-Numeric Columns\n",
    "# -------------------------------------------------------------------\n",
    "def detect_non_numeric_columns(csv_file):\n",
    "    \"\"\"\n",
    "    Reads csv_file and checks each column for non-numeric values.\n",
    "    Returns a dictionary mapping column names to the number of entries that become NaN\n",
    "    after converting to numeric (i.e. problematic entries).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file, engine='python', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "    non_numeric = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            converted = pd.to_numeric(df[col], errors='coerce')\n",
    "            additional_nans = converted.isna() & ~df[col].isna()\n",
    "            count = additional_nans.sum()\n",
    "            if count > 0:\n",
    "                non_numeric[col] = count\n",
    "        except Exception as e:\n",
    "            non_numeric[col] = str(e)\n",
    "    return non_numeric\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Main Processing: Iterate Through All CSV Files in the Google Drive Folder,\n",
    "# Clean Each One, and Detect Non-Numeric Columns.\n",
    "# -------------------------------------------------------------------\n",
    "folder_path = \"Dataset/groundtruth_raw/Raw_time_series\"  # Update if necessary.\n",
    "# Get list of all CSV files in the folder.\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSV files in {folder_path}\")\n",
    "\n",
    "# Iterate through each CSV file using tqdm.\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    # If the file is already cleaned (ends with _cleaned.csv), skip cleaning.\n",
    "    if not csv_file.endswith(\"_cleaned.csv\"):\n",
    "        cleaned_path = clean_csv(csv_file)\n",
    "    else:\n",
    "        cleaned_path = csv_file\n",
    "    print(f\"\\nDetecting non-numeric columns in: {cleaned_path}\")\n",
    "    problems = detect_non_numeric_columns(cleaned_path)\n",
    "    if problems:\n",
    "        print(\"Columns with non-numeric issues:\")\n",
    "        for col, count in problems.items():\n",
    "            print(f\"  {col}: {count}\")\n",
    "    else:\n",
    "        print(\"All columns are numeric (or properly handled).\")"
   ],
   "id": "c58f1448de607c5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 CSV files in Dataset/groundtruth_raw/Raw_time_series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: Dataset/groundtruth_raw/Raw_time_series\\deci_broad_timeSeries_raw_cleaned.csv\n",
      "\n",
      "Detecting non-numeric columns in: Dataset/groundtruth_raw/Raw_time_series\\deci_broad_timeSeries_raw_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  20%|██        | 1/5 [00:00<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with non-numeric issues:\n",
      "  Class: 21341\n",
      "  image_date: 21341\n",
      "Cleaned CSV saved to: Dataset/groundtruth_raw/Raw_time_series\\ever_coni_timeSeries_raw_cleaned.csv\n",
      "\n",
      "Detecting non-numeric columns in: Dataset/groundtruth_raw/Raw_time_series\\ever_coni_timeSeries_raw_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  40%|████      | 2/5 [00:01<00:02,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with non-numeric issues:\n",
      "  Class: 37067\n",
      "  image_date: 37067\n",
      "Cleaned CSV saved to: Dataset/groundtruth_raw/Raw_time_series\\larchCN_timeSeries_raw_cleaned.csv\n",
      "\n",
      "Detecting non-numeric columns in: Dataset/groundtruth_raw/Raw_time_series\\larchCN_timeSeries_raw_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  60%|██████    | 3/5 [00:01<00:01,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with non-numeric issues:\n",
      "  Class: 10340\n",
      "  image_date: 10340\n",
      "Cleaned CSV saved to: Dataset/groundtruth_raw/Raw_time_series\\larchJP_timeSeries_raw_cleaned.csv\n",
      "\n",
      "Detecting non-numeric columns in: Dataset/groundtruth_raw/Raw_time_series\\larchJP_timeSeries_raw_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files:  80%|████████  | 4/5 [00:03<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with non-numeric issues:\n",
      "  Class: 61542\n",
      "  image_date: 61542\n",
      "Cleaned CSV saved to: Dataset/groundtruth_raw/Raw_time_series\\shrubland_timeSeries_raw_cleaned.csv\n",
      "\n",
      "Detecting non-numeric columns in: Dataset/groundtruth_raw/Raw_time_series\\shrubland_timeSeries_raw_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files: 100%|██████████| 5/5 [00:03<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with non-numeric issues:\n",
      "  Class: 9020\n",
      "  image_date: 9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "concat into a single csv\n",
   "id": "d10ec951ca5dde87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:10:27.711875Z",
     "start_time": "2025-02-17T12:10:25.819560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def concatenate_groundtruth_files():\n",
    "    # 1. List of input CSV file paths\n",
    "    csv_files = [\n",
    "        r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\deci_broad_timeSeries_raw_cleaned.csv\",\n",
    "        r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\ever_coni_timeSeries_raw_cleaned.csv\",\n",
    "        r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\larchCN_timeSeries_raw_cleaned.csv\",\n",
    "        r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\larchJP_timeSeries_raw_cleaned.csv\",\n",
    "        r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\shrubland_timeSeries_raw_cleaned.csv\"\n",
    "    ]\n",
    "\n",
    "    # 2. Output CSV file path\n",
    "    output_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\Raw_time_series\\groundtruth_cleaned.csv\"\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    # 3. Read each CSV and store in a list\n",
    "    for path in csv_files:\n",
    "        print(f\"Reading: {path}\")\n",
    "        df = pd.read_csv(path, engine='python', on_bad_lines='skip')\n",
    "        print(f\"  Shape: {df.shape}, Columns: {df.columns.to_list()}\")\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    # 4. Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\nCombined DataFrame shape: {combined_df.shape}\")\n",
    "    print(\"Combined DataFrame columns:\", combined_df.columns.to_list())\n",
    "\n",
    "    # 5. Save the combined DataFrame to a single CSV file\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined CSV saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    concatenate_groundtruth_files()\n"
   ],
   "id": "9f626eab35003fa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\deci_broad_timeSeries_raw_cleaned.csv\n",
      "  Shape: (21341, 24), Columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "Reading: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\ever_coni_timeSeries_raw_cleaned.csv\n",
      "  Shape: (37067, 24), Columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "Reading: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\larchCN_timeSeries_raw_cleaned.csv\n",
      "  Shape: (10340, 24), Columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "Reading: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\larchJP_timeSeries_raw_cleaned.csv\n",
      "  Shape: (61542, 24), Columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "Reading: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\shrubland_timeSeries_raw_cleaned.csv\n",
      "  Shape: (9020, 24), Columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "\n",
      "Combined DataFrame shape: (139310, 24)\n",
      "Combined DataFrame columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "Combined CSV saved to: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\Raw_time_series\\groundtruth_cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check for quality of csv if any errors or wrongful inputations",
   "id": "424d426727fc1509"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:10:51.113165Z",
     "start_time": "2025-02-17T12:10:49.967734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_groundtruth_cleaned(csv_path):\n",
    "    print(f\"Reading CSV from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, engine='python', on_bad_lines='skip')\n",
    "\n",
    "    # 1. Print basic info\n",
    "    print(\"\\n--- Basic Info ---\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(df.info())  # includes dtypes & non-null counts\n",
    "\n",
    "    # 2. Identify numeric vs. non-numeric columns\n",
    "    print(\"\\n--- Numeric vs Non-Numeric Columns ---\")\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"Column '{col}' is numeric.\")\n",
    "        else:\n",
    "            print(f\"Column '{col}' is non-numeric.\")\n",
    "\n",
    "    # 3. (Optional) If you have an expected set of columns, compare them\n",
    "    # Example of typical columns for your time-series data\n",
    "    expected_cols = {\n",
    "        \"AOT\",\"B1\",\"B11\",\"B12\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\n",
    "        \"EVI\",\"NDVI\",\"NDWI\",\"TCI_B\",\"TCI_G\",\"TCI_R\",\"WVP\",\"image_date\",\"longitude\",\"latitude\",\"Class\"\n",
    "    }\n",
    "    found_cols = set(df.columns)\n",
    "    missing = expected_cols - found_cols\n",
    "    extra = found_cols - expected_cols\n",
    "\n",
    "    print(\"\\n--- Column Comparison ---\")\n",
    "    if missing:\n",
    "        print(f\"Missing columns (expected but not found): {missing}\")\n",
    "    else:\n",
    "        print(\"No missing columns from the expected set.\")\n",
    "    if extra:\n",
    "        print(f\"Extra columns (found but not in expected set): {extra}\")\n",
    "    else:\n",
    "        print(\"No extra columns beyond the expected set.\")\n",
    "\n",
    "    # 4. Check the 'Class' column if present\n",
    "    if \"Class\" in df.columns:\n",
    "        unique_classes = df[\"Class\"].unique()\n",
    "        print(\"\\n--- Unique Classes ---\")\n",
    "        print(f\"Found {len(unique_classes)} unique class(es): {unique_classes}\")\n",
    "    else:\n",
    "        print(\"\\nNo 'Class' column found in the DataFrame.\")\n",
    "\n",
    "    print(\"\\n--- Done Checking Groundtruth Cleaned CSV ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\Raw_time_series\\groundtruth_cleaned.csv\"\n",
    "    check_groundtruth_cleaned(csv_path)\n"
   ],
   "id": "d06f0b3a0705fd48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV from: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\Raw_time_series\\groundtruth_cleaned.csv\n",
      "\n",
      "--- Basic Info ---\n",
      "DataFrame shape: (139310, 24)\n",
      "Columns: ['AOT', 'B1', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'Class', 'EVI', 'NDVI', 'NDWI', 'TCI_B', 'TCI_G', 'TCI_R', 'WVP', 'image_date', 'longitude', 'latitude']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139310 entries, 0 to 139309\n",
      "Data columns (total 24 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   AOT         139310 non-null  int64  \n",
      " 1   B1          139310 non-null  int64  \n",
      " 2   B11         139310 non-null  int64  \n",
      " 3   B12         139310 non-null  int64  \n",
      " 4   B2          139310 non-null  int64  \n",
      " 5   B3          139310 non-null  int64  \n",
      " 6   B4          139310 non-null  int64  \n",
      " 7   B5          139310 non-null  int64  \n",
      " 8   B6          139310 non-null  int64  \n",
      " 9   B7          139310 non-null  int64  \n",
      " 10  B8          139310 non-null  int64  \n",
      " 11  B8A         139310 non-null  int64  \n",
      " 12  B9          139310 non-null  int64  \n",
      " 13  Class       139310 non-null  object \n",
      " 14  EVI         139310 non-null  float64\n",
      " 15  NDVI        139310 non-null  float64\n",
      " 16  NDWI        139310 non-null  float64\n",
      " 17  TCI_B       139310 non-null  int64  \n",
      " 18  TCI_G       139310 non-null  int64  \n",
      " 19  TCI_R       139310 non-null  int64  \n",
      " 20  WVP         139310 non-null  int64  \n",
      " 21  image_date  139310 non-null  object \n",
      " 22  longitude   139310 non-null  float64\n",
      " 23  latitude    139310 non-null  float64\n",
      "dtypes: float64(5), int64(17), object(2)\n",
      "memory usage: 25.5+ MB\n",
      "None\n",
      "\n",
      "--- Numeric vs Non-Numeric Columns ---\n",
      "Column 'AOT' is numeric.\n",
      "Column 'B1' is numeric.\n",
      "Column 'B11' is numeric.\n",
      "Column 'B12' is numeric.\n",
      "Column 'B2' is numeric.\n",
      "Column 'B3' is numeric.\n",
      "Column 'B4' is numeric.\n",
      "Column 'B5' is numeric.\n",
      "Column 'B6' is numeric.\n",
      "Column 'B7' is numeric.\n",
      "Column 'B8' is numeric.\n",
      "Column 'B8A' is numeric.\n",
      "Column 'B9' is numeric.\n",
      "Column 'Class' is non-numeric.\n",
      "Column 'EVI' is numeric.\n",
      "Column 'NDVI' is numeric.\n",
      "Column 'NDWI' is numeric.\n",
      "Column 'TCI_B' is numeric.\n",
      "Column 'TCI_G' is numeric.\n",
      "Column 'TCI_R' is numeric.\n",
      "Column 'WVP' is numeric.\n",
      "Column 'image_date' is non-numeric.\n",
      "Column 'longitude' is numeric.\n",
      "Column 'latitude' is numeric.\n",
      "\n",
      "--- Column Comparison ---\n",
      "No missing columns from the expected set.\n",
      "No extra columns beyond the expected set.\n",
      "\n",
      "--- Unique Classes ---\n",
      "Found 5 unique class(es): ['deci_broad' 'ever_coni' 'larch_CN' 'larch_JP' 'shrubland']\n",
      "\n",
      "--- Done Checking Groundtruth Cleaned CSV ---\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transformer code adopted to our groundtruth",
   "id": "a886428c40465af2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:43:55.804367Z",
     "start_time": "2025-02-17T12:18:42.249698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Set the Device (CUDA)\n",
    "# -------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name())\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Dataset Class for Groundtruth CSV\n",
    "# -------------------------------\n",
    "class GroundtruthDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads a groundtruth CSV with columns:\n",
    "      [AOT, B1, B11, B12, B2, B3, B4, B5, B6, B7, B8, B8A, B9,\n",
    "       EVI, NDVI, NDWI, TCI_B, TCI_G, TCI_R, WVP,\n",
    "       Class, image_date, longitude, latitude]\n",
    "\n",
    "    Creates time-series samples by grouping on an 'id' column,\n",
    "    which is made from rounding (longitude, latitude).\n",
    "    Each sample has shape (12, 20) if desired_length=12.\n",
    "    The dataset also stores the 'Class' plus the (latitude, longitude)\n",
    "    for each group for later reference, but these are *not* used\n",
    "    as Transformer inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, desired_length=12, feature_cols=None):\n",
    "        print(f\"\\nReading CSV file: {csv_path}\")\n",
    "        self.df = pd.read_csv(csv_path, parse_dates=[\"image_date\"])\n",
    "\n",
    "        # If 'id' column not present, create from (longitude, latitude).\n",
    "        if 'id' not in self.df.columns:\n",
    "            if 'longitude' not in self.df.columns or 'latitude' not in self.df.columns:\n",
    "                raise ValueError(\"CSV must contain 'longitude' and 'latitude' columns.\")\n",
    "            # Create an 'id' by rounding to 6 decimals\n",
    "            self.df['id'] = (self.df['longitude'].round(6).astype(str)\n",
    "                             + \"_\"\n",
    "                             + self.df['latitude'].round(6).astype(str))\n",
    "\n",
    "        # By default, we have 20 feature columns for the Transformer:\n",
    "        if feature_cols is None:\n",
    "            self.feature_cols = [\n",
    "                \"AOT\",\"B1\",\"B11\",\"B12\",\"B2\",\"B3\",\"B4\",\"B5\",\n",
    "                \"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"EVI\",\"NDVI\",\"NDWI\",\n",
    "                \"TCI_B\",\"TCI_G\",\"TCI_R\",\"WVP\"\n",
    "            ]\n",
    "        else:\n",
    "            self.feature_cols = feature_cols\n",
    "\n",
    "        self.desired_length = desired_length\n",
    "\n",
    "        # We'll store: time-series (T=12, D=20), plus the first row's (lat, lon, Class).\n",
    "        self.samples = []\n",
    "        self.coords = []    # list of (latitude, longitude)\n",
    "        self.classes = []   # list of class labels\n",
    "\n",
    "        groups = self.df.groupby('id', sort=False)\n",
    "        for group_id, group in groups:\n",
    "            group_sorted = group.sort_values('image_date')\n",
    "\n",
    "            # Convert the 20 feature columns to float array:\n",
    "            arr = group_sorted[self.feature_cols].to_numpy(dtype=float)  # shape (T, D)\n",
    "            T, D = arr.shape\n",
    "\n",
    "            # Pad or truncate to 'desired_length':\n",
    "            if T < desired_length:\n",
    "                # pad with the last row\n",
    "                pad = np.tile(arr[-1:], (desired_length - T, 1))\n",
    "                arr = np.vstack([arr, pad])\n",
    "            elif T > desired_length:\n",
    "                arr = arr[:desired_length]\n",
    "\n",
    "            # Convert to PyTorch tensor\n",
    "            ts_tensor = torch.tensor(arr, dtype=torch.float)  # shape: (desired_length, D)\n",
    "\n",
    "            # We'll store a dummy label for unsupervised training\n",
    "            label = 0\n",
    "\n",
    "            # Take the first row's lat/lon/class for reference:\n",
    "            lat_0 = group_sorted.iloc[0]['latitude']\n",
    "            lon_0 = group_sorted.iloc[0]['longitude']\n",
    "            class_0 = str(group_sorted.iloc[0]['Class'])  # store as string\n",
    "\n",
    "            self.samples.append((ts_tensor, label))\n",
    "            self.coords.append((lat_0, lon_0))\n",
    "            self.classes.append(class_0)\n",
    "\n",
    "        print(f\"Constructed {len(self.samples)} time series samples from {csv_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ts_data, label = self.samples[idx]\n",
    "        coord = self.coords[idx]\n",
    "        class_str = self.classes[idx]\n",
    "        return ts_data, torch.tensor(label, dtype=torch.long), coord, class_str\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Transformer Components\n",
    "# -------------------------------\n",
    "class SimplePosEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple positional encoding that adds a sin(...) offset\n",
    "    based on (time, feature_index). Not the standard approach,\n",
    "    but works for demonstration.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        B, T, D = x.shape\n",
    "        for t in range(T):\n",
    "            for i in range(D):\n",
    "                offset = math.sin((t+1)*(i+1)/30.0)\n",
    "                x[:, t, i] += offset\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        B, T, D = x.shape\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        # Reshape to (B, num_heads, T, head_dim)\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, num_heads, T, T)\n",
    "        out = torch.matmul(attn, V)          # (B, num_heads, T, head_dim)\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.out_linear(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=1, ff_hidden=64):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        x_attn = self.attn(x)\n",
    "        x = x + x_attn\n",
    "        x = self.norm1(x)\n",
    "        x_ff = self.ff(x)\n",
    "        x = x + x_ff\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class MiniTransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal Transformer that does:\n",
    "      - Simple positional encoding\n",
    "      - N layers of (MHSA + feedforward)\n",
    "      - Mean-pool across time\n",
    "      - Output a 2-class logit (dummy usage)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads=1, ff_hidden=64, num_layers=2, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.posenc = SimplePosEncoding()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, ff_hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x, return_latent=False):\n",
    "        # x: (B, T, D)\n",
    "        x = self.posenc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        # Mean-pool across time dimension\n",
    "        x_mean = x.mean(dim=1)  # shape (B, d_model)\n",
    "        logits = self.classifier(x_mean)\n",
    "        if return_latent:\n",
    "            # Return the final latent vector (x_mean)\n",
    "            return logits, x_mean\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training & Latent Extraction\n",
    "# -------------------------------\n",
    "def train_transformer_on_groundtruth(csv_path, epochs=50, save_folder=\".\"):\n",
    "    print(f\"\\n--- Training on groundtruth CSV: {csv_path} ---\")\n",
    "    dataset = GroundtruthDataset(csv_path, desired_length=12)\n",
    "    ds_len = len(dataset)\n",
    "    print(\"Dataset contains\", ds_len, \"samples.\")\n",
    "    if ds_len < 2:\n",
    "        print(\"Not enough samples to split. Using entire dataset for both train & test.\")\n",
    "        train_ds = dataset\n",
    "        test_ds = dataset\n",
    "    else:\n",
    "        train_size = int(0.8 * ds_len)\n",
    "        test_size = ds_len - train_size\n",
    "        train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    d_model = len(dataset.feature_cols)  # Should be 20 features\n",
    "    seq_len = 12\n",
    "    print(f\"Using d_model={d_model}, seq_len={seq_len}. Training for {epochs} epochs.\")\n",
    "    model = MiniTransformerClassifier(d_model=d_model, num_heads=1, ff_hidden=64, num_layers=2, n_classes=2)\n",
    "    model.to(device)\n",
    "    print(\"Model is on device:\", next(model.parameters()).device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X, _, _, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} training\", leave=False):\n",
    "            X = X.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            # Dummy label = 0\n",
    "            labels = torch.zeros(X.size(0), dtype=torch.long, device=device)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "        avg_loss = total_loss / len(train_ds)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, _, _, _ in tqdm(test_loader, desc=f\"Epoch {epoch}/{epochs} testing\", leave=False):\n",
    "                X = X.to(device)\n",
    "                logits = model(X)\n",
    "                labels = torch.zeros(X.size(0), dtype=torch.long, device=device)\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item() * X.size(0)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == 0).sum().item()\n",
    "                total += X.size(0)\n",
    "        avg_test_loss = test_loss / len(test_ds)\n",
    "        test_acc = correct/total if total>0 else 1.0\n",
    "        print(f\"Epoch {epoch}/{epochs}, Train Loss={avg_loss:.4f}, Test Loss={avg_test_loss:.4f}, Dummy Acc={test_acc:.2f}\")\n",
    "\n",
    "    # Output folder for weights & latents\n",
    "    output_folder = os.path.join(save_folder, \"transformer_output_groundtruth\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save model weights\n",
    "    base_name = os.path.basename(csv_path).split(\".\")[0]\n",
    "    save_path = os.path.join(output_folder, base_name + \"_transformer.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"Model weights saved to:\", save_path)\n",
    "\n",
    "    # Extract latents for entire dataset\n",
    "    model.eval()\n",
    "    all_latents = []\n",
    "    all_coords = dataset.coords\n",
    "    all_classes = dataset.classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "        for X, _, _, _ in tqdm(loader, desc=\"Extracting latents\", leave=True):\n",
    "            X = X.to(device)\n",
    "            _, latent = model(X, return_latent=True)\n",
    "            all_latents.append(latent.cpu().numpy())\n",
    "    all_latents = np.concatenate(all_latents, axis=0)\n",
    "    print(\"Extracted latent shape:\", all_latents.shape)\n",
    "\n",
    "    # Build a final DataFrame with 20 latent columns + lat/lon + Class\n",
    "    latent_df = pd.DataFrame(all_latents)  # columns = 0..19\n",
    "    lat_list = [coord[0] for coord in all_coords]\n",
    "    lon_list = [coord[1] for coord in all_coords]\n",
    "\n",
    "    latent_df[\"latitude\"] = lat_list\n",
    "    latent_df[\"longitude\"] = lon_list\n",
    "    latent_df[\"Class\"] = all_classes\n",
    "\n",
    "    # Save final CSV\n",
    "    latent_csv_path = os.path.join(output_folder, base_name + \"_latent_features.csv\")\n",
    "    latent_df.to_csv(latent_csv_path, index=False)\n",
    "    print(\"Latent features with co  ordinates + Class saved to:\", latent_csv_path)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Main Execution (Single CSV)\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    csv_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\groundtruth_cleaned_final.csv\"\n",
    "    train_transformer_on_groundtruth(csv_path, epochs=50, save_folder=\".\")\n"
   ],
   "id": "93bc788fd7ba77b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA RTX 2000 Ada Generation\n",
      "\n",
      "--- Training on groundtruth CSV: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\groundtruth_cleaned_final.csv ---\n",
      "\n",
      "Reading CSV file: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\groundtruth_cleaned_final.csv\n",
      "Constructed 16464 time series samples from C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\groundtruth_cleaned_final.csv\n",
      "Dataset contains 16464 samples.\n",
      "Using d_model=20, seq_len=12. Training for 50 epochs.\n",
      "Model is on device: cuda:0\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss=0.0053, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss=0.0000, Test Loss=0.0000, Dummy Acc=1.00\n",
      "Model weights saved to: .\\transformer_output_groundtruth\\groundtruth_cleaned_final_transformer.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting latents: 100%|██████████| 1029/1029 [00:13<00:00, 75.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted latent shape: (16464, 20)\n",
      "Latent features with coordinates + Class saved to: .\\transformer_output_groundtruth\\groundtruth_cleaned_final_latent_features.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check CSV quality of both groundtruth and transformer output",
   "id": "376c32f39cabf40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:50:03.069917Z",
     "start_time": "2025-02-17T12:50:00.004676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of CSV file paths\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\jmm267\\Downloads\\Binbin\\transformer_output_groundtruth\\groundtruth_cleaned_final_latent_features.csv\",\n",
    "    r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part1.csv\",\n",
    "    r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part2.csv\",\n",
    "    r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part3.csv\"\n",
    "]\n",
    "\n",
    "total_rows = 0\n",
    "\n",
    "for file in file_paths:\n",
    "    if os.path.exists(file):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"File: {file}\")\n",
    "\n",
    "        # Print columns and their data types\n",
    "        print(\"Columns and data types:\")\n",
    "        print(df.dtypes)\n",
    "\n",
    "        # Print number of rows in the file\n",
    "        num_rows = len(df)\n",
    "        print(f\"Number of rows: {num_rows}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        total_rows += num_rows\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "print(f\"Total combined rows across all files: {total_rows}\")\n"
   ],
   "id": "4e4cef430fce2227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: C:\\Users\\jmm267\\Downloads\\Binbin\\transformer_output_groundtruth\\groundtruth_cleaned_final_latent_features.csv\n",
      "Columns and data types:\n",
      "0            float64\n",
      "1            float64\n",
      "2            float64\n",
      "3            float64\n",
      "4            float64\n",
      "5            float64\n",
      "6            float64\n",
      "7            float64\n",
      "8            float64\n",
      "9            float64\n",
      "10           float64\n",
      "11           float64\n",
      "12           float64\n",
      "13           float64\n",
      "14           float64\n",
      "15           float64\n",
      "16           float64\n",
      "17           float64\n",
      "18           float64\n",
      "19           float64\n",
      "latitude     float64\n",
      "longitude    float64\n",
      "Class         object\n",
      "dtype: object\n",
      "Number of rows: 16464\n",
      "------------------------------------------------------------\n",
      "File: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part1.csv\n",
      "Columns and data types:\n",
      "0                float64\n",
      "1                float64\n",
      "2                float64\n",
      "3                float64\n",
      "4                float64\n",
      "5                float64\n",
      "6                float64\n",
      "7                float64\n",
      "8                float64\n",
      "9                float64\n",
      "10               float64\n",
      "11               float64\n",
      "12               float64\n",
      "13               float64\n",
      "14               float64\n",
      "15               float64\n",
      "16               float64\n",
      "17               float64\n",
      "18               float64\n",
      "19               float64\n",
      "latitude         float64\n",
      "longitude        float64\n",
      "cluster_label      int64\n",
      "dtype: object\n",
      "Number of rows: 1048576\n",
      "------------------------------------------------------------\n",
      "File: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part2.csv\n",
      "Columns and data types:\n",
      "0                float64\n",
      "1                float64\n",
      "2                float64\n",
      "3                float64\n",
      "4                float64\n",
      "5                float64\n",
      "6                float64\n",
      "7                float64\n",
      "8                float64\n",
      "9                float64\n",
      "10               float64\n",
      "11               float64\n",
      "12               float64\n",
      "13               float64\n",
      "14               float64\n",
      "15               float64\n",
      "16               float64\n",
      "17               float64\n",
      "18               float64\n",
      "19               float64\n",
      "latitude         float64\n",
      "longitude        float64\n",
      "cluster_label      int64\n",
      "dtype: object\n",
      "Number of rows: 1048576\n",
      "------------------------------------------------------------\n",
      "File: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part3.csv\n",
      "Columns and data types:\n",
      "0                float64\n",
      "1                float64\n",
      "2                float64\n",
      "3                float64\n",
      "4                float64\n",
      "5                float64\n",
      "6                float64\n",
      "7                float64\n",
      "8                float64\n",
      "9                float64\n",
      "10               float64\n",
      "11               float64\n",
      "12               float64\n",
      "13               float64\n",
      "14               float64\n",
      "15               float64\n",
      "16               float64\n",
      "17               float64\n",
      "18               float64\n",
      "19               float64\n",
      "latitude         float64\n",
      "longitude        float64\n",
      "cluster_label      int64\n",
      "dtype: object\n",
      "Number of rows: 108800\n",
      "------------------------------------------------------------\n",
      "Total combined rows across all files: 2222416\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Running the classifer",
   "id": "9cf9aa6c581e6e44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:57:43.658275Z",
     "start_time": "2025-02-17T12:57:08.797072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def main():\n",
    "    ###############################################################################\n",
    "    # 1. Load Groundtruth Latent Features\n",
    "    ###############################################################################\n",
    "    groundtruth_csv = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\transformer_output_groundtruth\\groundtruth_cleaned_final_latent_features.csv\"\n",
    "    print(f\"Reading groundtruth CSV from: {groundtruth_csv}\")\n",
    "    df_gt = pd.read_csv(groundtruth_csv)\n",
    "    print(\"Groundtruth shape:\", df_gt.shape)\n",
    "    print(\"Groundtruth columns:\", df_gt.columns.tolist())\n",
    "\n",
    "    # 20 latent features are columns 0..19\n",
    "    feature_cols = [str(i) for i in range(20)]\n",
    "    # The class label is in 'Class'\n",
    "    # The lat/lon columns are 'latitude','longitude', but not used for training\n",
    "\n",
    "    ###############################################################################\n",
    "    # 2. Encode the Class & Prepare Train/Test\n",
    "    ###############################################################################\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_gt['class_label'] = label_encoder.fit_transform(df_gt['Class'])\n",
    "    print(\"Classes found:\", list(label_encoder.classes_))\n",
    "\n",
    "    X = df_gt[feature_cols].values  # shape (N, 20)\n",
    "    y = df_gt['class_label'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # 3. Train a RandomForest Classifier\n",
    "    ###############################################################################\n",
    "    print(\"Training RandomForestClassifier on groundtruth data...\")\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate quickly on test set\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    print(f\"Test accuracy on groundtruth: {test_acc:.3f}\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # 4. Load DEC-Clustered Parts & Concatenate\n",
    "    ###############################################################################\n",
    "    dec_folder = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\"\n",
    "    part_files = [\n",
    "        \"combined_DEC_clustered_part1.csv\",\n",
    "        \"combined_DEC_clustered_part2.csv\",\n",
    "        \"combined_DEC_clustered_part3.csv\"\n",
    "    ]\n",
    "    all_dec_dfs = []\n",
    "    for f in part_files:\n",
    "        path = os.path.join(dec_folder, f)\n",
    "        print(f\"Reading DEC part: {path}\")\n",
    "        df_part = pd.read_csv(path)\n",
    "        all_dec_dfs.append(df_part)\n",
    "    df_dec = pd.concat(all_dec_dfs, ignore_index=True)\n",
    "    print(\"DEC dataset shape (combined):\", df_dec.shape)\n",
    "    print(\"DEC columns:\", df_dec.columns.tolist())\n",
    "\n",
    "    # The DEC dataset has 20 columns named \"0..19\", plus 'latitude','longitude','cluster_label'.\n",
    "\n",
    "    ###############################################################################\n",
    "    # 5. Predict Class Probabilities for the DEC Data\n",
    "    ###############################################################################\n",
    "    # We'll apply the same 20 columns as features:\n",
    "    X_dec = df_dec[feature_cols].values  # shape (M, 20)\n",
    "\n",
    "    print(\"Predicting class probabilities on DEC data...\")\n",
    "    # predict_proba returns shape (M, #classes)\n",
    "    y_proba = clf.predict_proba(X_dec)\n",
    "\n",
    "    # We'll also pick the argmax as predicted class\n",
    "    y_pred = np.argmax(y_proba, axis=1)  # shape (M,)\n",
    "\n",
    "    # Create columns for each class probability\n",
    "    # E.g. \"prob_deci_broad\", \"prob_ever_coni\", ...\n",
    "    class_names = list(label_encoder.classes_)  # e.g. [\"deci_broad\",\"ever_coni\",\"larch_CN\",\"larch_JP\",\"shrubland\"]\n",
    "    prob_cols = [f\"prob_{cn}\" for cn in class_names]\n",
    "\n",
    "    df_probs = pd.DataFrame(y_proba, columns=prob_cols)\n",
    "    df_pred_class = pd.Series(label_encoder.inverse_transform(y_pred), name=\"predicted_class\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # 6. Merge Probabilities with DEC Data & Save\n",
    "    ###############################################################################\n",
    "    # We can either store them in a single big DataFrame or chunk them again.\n",
    "    # For simplicity, we'll store them in one DataFrame in memory:\n",
    "    result_df = pd.concat([df_dec, df_probs, df_pred_class], axis=1)\n",
    "\n",
    "    # The result has columns:\n",
    "    #   0..19, latitude, longitude, cluster_label,\n",
    "    #   prob_deci_broad, prob_ever_coni, prob_larch_CN, prob_larch_JP, prob_shrubland,\n",
    "    #   predicted_class\n",
    "    print(\"Final result shape:\", result_df.shape)\n",
    "    print(\"Sample rows:\\n\", result_df.head(5))\n",
    "\n",
    "    # Save to a single CSV. (2.2 million rows can be large but typically feasible.)\n",
    "    # If memory is an issue, consider chunked approach or partitioned saving.\n",
    "    output_csv = os.path.join(dec_folder, \"combined_DEC_clustered_with_classProbs.csv\")\n",
    "    print(f\"Saving final DEC with class probabilities to: {output_csv}\")\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "e26161b8479c8037",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading groundtruth CSV from: C:\\Users\\jmm267\\Downloads\\Binbin\\transformer_output_groundtruth\\groundtruth_cleaned_final_latent_features.csv\n",
      "Groundtruth shape: (16464, 23)\n",
      "Groundtruth columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', 'latitude', 'longitude', 'Class']\n",
      "Classes found: ['deci_broad', 'ever_coni', 'larch_CN', 'larch_JP', 'shrubland']\n",
      "Train size: 13171, Test size: 3293\n",
      "Training RandomForestClassifier on groundtruth data...\n",
      "Test accuracy on groundtruth: 0.950\n",
      "Reading DEC part: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part1.csv\n",
      "Reading DEC part: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part2.csv\n",
      "Reading DEC part: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part3.csv\n",
      "DEC dataset shape (combined): (2205952, 23)\n",
      "DEC columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', 'latitude', 'longitude', 'cluster_label']\n",
      "Predicting class probabilities on DEC data...\n",
      "Final result shape: (2205952, 29)\n",
      "Sample rows:\n",
      "           0         1         2         3          4         5         6  \\\n",
      "0  0.155357  0.286328  0.393404  0.614512 -13.277342 -0.116932  0.244814   \n",
      "1  0.151448  0.289241  0.390746  0.617897 -13.285116 -0.115736  0.244864   \n",
      "2  0.151448  0.289241  0.390746  0.617897 -13.285116 -0.115736  0.244864   \n",
      "3  0.154539  0.290430  0.393342  0.620422 -13.284363 -0.124646  0.240445   \n",
      "4  0.153474  0.291959  0.391579  0.622427 -13.276670 -0.125364  0.236160   \n",
      "\n",
      "          7         8         9  ...        19  latitude   longitude  \\\n",
      "0  0.948458  1.034857 -0.511455  ... -0.406401  33.62731  107.568449   \n",
      "1  0.946321  1.030107 -0.509817  ... -0.399068  33.62731  107.568539   \n",
      "2  0.946321  1.030107 -0.509817  ... -0.399068  33.62731  107.568629   \n",
      "3  0.945105  1.030176 -0.505772  ... -0.398573  33.62731  107.568719   \n",
      "4  0.947631  1.035075 -0.506821  ... -0.401849  33.62731  107.568809   \n",
      "\n",
      "   cluster_label  prob_deci_broad  prob_ever_coni  prob_larch_CN  \\\n",
      "0              2             0.39           0.135           0.19   \n",
      "1              2             0.39           0.135           0.19   \n",
      "2              2             0.39           0.135           0.19   \n",
      "3              2             0.39           0.135           0.19   \n",
      "4              2             0.39           0.135           0.19   \n",
      "\n",
      "   prob_larch_JP  prob_shrubland  predicted_class  \n",
      "0          0.185             0.1       deci_broad  \n",
      "1          0.185             0.1       deci_broad  \n",
      "2          0.185             0.1       deci_broad  \n",
      "3          0.185             0.1       deci_broad  \n",
      "4          0.185             0.1       deci_broad  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "Saving final DEC with class probabilities to: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_with_classProbs.csv\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train new classifier.",
   "id": "f61e8035fcdee5b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:22:54.121758Z",
     "start_time": "2025-02-17T13:21:18.268130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For progress bars\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from tqdm.keras import TqdmCallback\n",
    "except ImportError:\n",
    "    print(\"Please install tqdm >= 4.36.0 with `pip install tqdm` to use TqdmCallback for Keras.\")\n",
    "    raise\n",
    "\n",
    "# For neural network classification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##############################################\n",
    "# 0. Configure GPU if available (CUDA usage) #\n",
    "##############################################\n",
    "print(\"=== Checking for GPU (CUDA) availability ===\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable dynamic memory growth on all found GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) found: {[gpu.name for gpu in gpus]}\")\n",
    "        print(\"Successfully set GPU memory growth. TensorFlow will use GPU by default.\\n\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Could not set GPU memory growth:\", e, \"\\n\")\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\\n\")\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 1. Load and inspect the groundtruth data  #\n",
    "#############################################\n",
    "groundtruth_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\groundtruth_cleaned_final_latent_features.csv\"\n",
    "\n",
    "print(\"=== Loading Groundtruth Data ===\")\n",
    "print(f\"Reading groundtruth data from:\\n  {groundtruth_path}\")\n",
    "df_groundtruth = pd.read_csv(groundtruth_path)\n",
    "print(f\"Loaded groundtruth data with shape: {df_groundtruth.shape}\")\n",
    "print(\"First few rows:\\n\", df_groundtruth.head(), \"\\n\")\n",
    "\n",
    "# Separate features (X) and labels (y).\n",
    "X = df_groundtruth.iloc[:, 0:20].values\n",
    "y = df_groundtruth['Class'].values\n",
    "\n",
    "# Encode class labels to numeric\n",
    "print(\"Encoding class labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Scale the latent features\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/Validation split\n",
    "print(\"Splitting data into train/validation sets...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "print(\"Train set shape:\", X_train.shape, \"Val set shape:\", X_val.shape, \"\\n\")\n",
    "\n",
    "##############################################\n",
    "# 2. Build and train a neural network model  #\n",
    "##############################################\n",
    "print(\"=== Building and Training the Model ===\")\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"Compiling model...\")\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Starting training for 100 epochs...\\n\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    verbose=0,  # Turn off default Keras logging\n",
    "    callbacks=[TqdmCallback(verbose=1)]  # Use TQDM progress bar\n",
    ")\n",
    "\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final Training Accuracy:   {train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "################################################\n",
    "# 3. Load and combine the DEC-clustered files  #\n",
    "################################################\n",
    "dec_part1_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part1.csv\"\n",
    "dec_part2_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part2.csv\"\n",
    "dec_part3_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\combined_DEC_clustered_part3.csv\"\n",
    "\n",
    "dec_files = [dec_part1_path, dec_part2_path, dec_part3_path]\n",
    "\n",
    "print(\"=== Loading and Concatenating DEC Data ===\")\n",
    "df_list = []\n",
    "for file_path in tqdm(dec_files, desc=\"Reading DEC CSVs\"):\n",
    "    df_part = pd.read_csv(file_path)\n",
    "    df_list.append(df_part)\n",
    "\n",
    "df_dec_combined = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "print(f\"All DEC files loaded and concatenated. Combined shape: {df_dec_combined.shape}\")\n",
    "print(\"First few rows of the combined DEC data:\\n\", df_dec_combined.head(), \"\\n\")\n",
    "\n",
    "##############################################\n",
    "# 4. Apply the trained classifier to DEC data\n",
    "##############################################\n",
    "print(\"=== Predicting Classes for DEC Data ===\")\n",
    "X_dec = df_dec_combined.iloc[:, 0:20].values\n",
    "\n",
    "# Scale them using the same scaler\n",
    "X_dec_scaled = scaler.transform(X_dec)\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "y_dec_pred_probs = model.predict(X_dec_scaled)\n",
    "y_dec_pred_classes = np.argmax(y_dec_pred_probs, axis=1)\n",
    "\n",
    "# Convert numeric predictions back to original class names\n",
    "dec_pred_labels = label_encoder.inverse_transform(y_dec_pred_classes)\n",
    "df_dec_combined['predicted_class'] = dec_pred_labels\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\DEC_with_predictions.csv\"\n",
    "print(f\"Saving predictions to:\\n  {output_path}\")\n",
    "df_dec_combined.to_csv(output_path, index=False)\n",
    "print(\"Done!\")\n"
   ],
   "id": "7fe26b9a01382bb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking for GPU (CUDA) availability ===\n",
      "No GPU found. Running on CPU.\n",
      "\n",
      "=== Loading Groundtruth Data ===\n",
      "Reading groundtruth data from:\n",
      "  C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\groundtruth_cleaned_final_latent_features.csv\n",
      "Loaded groundtruth data with shape: (16464, 23)\n",
      "First few rows:\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0  0.303286  0.850955  0.057265 -1.085985  0.244422  1.693576  2.825499   \n",
      "1  0.303553  0.855530  0.056125 -1.088880  0.244643  1.693512  2.832759   \n",
      "2  0.302698  0.856721  0.055590 -1.087381  0.242954  1.690936  2.831756   \n",
      "3  0.295183  0.854956  0.058925 -1.088144  0.250835  1.689031  2.831896   \n",
      "4  0.296416  0.864984  0.057353 -1.086355  0.250660  1.688048  2.831185   \n",
      "\n",
      "          7         8         9  ...        13         14        15        16  \\\n",
      "0 -2.474173  1.149435  1.074792  ... -0.097846 -11.351932 -0.426686  0.514707   \n",
      "1 -2.475309  1.146067  1.072967  ... -0.097587 -11.348213 -0.428438  0.516408   \n",
      "2 -2.473271  1.145863  1.075170  ... -0.097942 -11.347784 -0.428945  0.518795   \n",
      "3 -2.493361  1.139639  1.073070  ... -0.101583 -11.348394 -0.422263  0.513692   \n",
      "4 -2.498165  1.133687  1.074138  ... -0.102374 -11.344287 -0.427436  0.516350   \n",
      "\n",
      "         17        18        19   latitude   longitude       Class  \n",
      "0  2.084976 -0.216011 -0.261418  33.699714  107.617228  deci_broad  \n",
      "1  2.087377 -0.213417 -0.262339  33.699714  107.617318  deci_broad  \n",
      "2  2.087534 -0.210600 -0.262373  33.699714  107.617407  deci_broad  \n",
      "3  2.095601 -0.209220 -0.262113  33.699804  107.617228  deci_broad  \n",
      "4  2.098054 -0.205333 -0.264368  33.699804  107.617318  deci_broad  \n",
      "\n",
      "[5 rows x 23 columns] \n",
      "\n",
      "Encoding class labels...\n",
      "Scaling features...\n",
      "Splitting data into train/validation sets...\n",
      "Train set shape: (13171, 20) Val set shape: (3293, 20) \n",
      "\n",
      "=== Building and Training the Model ===\n",
      "Compiling model...\n",
      "Starting training for 100 epochs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmm267\\Downloads\\Binbin\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ba601d8af9043a4b74a03098fb55ded"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9debe6be8e8d4f3ea3f47b5bbd8ef54c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Final Training Accuracy:   0.9579\n",
      "Final Validation Accuracy: 0.9602\n",
      "\n",
      "=== Loading and Concatenating DEC Data ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading DEC CSVs: 100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DEC files loaded and concatenated. Combined shape: (2205952, 23)\n",
      "First few rows of the combined DEC data:\n",
      "           0         1         2         3          4         5         6  \\\n",
      "0  0.155357  0.286328  0.393404  0.614512 -13.277342 -0.116932  0.244814   \n",
      "1  0.151448  0.289241  0.390746  0.617897 -13.285116 -0.115736  0.244864   \n",
      "2  0.151448  0.289241  0.390746  0.617897 -13.285116 -0.115736  0.244864   \n",
      "3  0.154539  0.290430  0.393342  0.620422 -13.284363 -0.124646  0.240445   \n",
      "4  0.153474  0.291959  0.391579  0.622427 -13.276670 -0.125364  0.236160   \n",
      "\n",
      "          7         8         9  ...        13        14        15        16  \\\n",
      "0  0.948458  1.034857 -0.511455  ...  0.885650 -0.656209  3.106227 -0.126195   \n",
      "1  0.946321  1.030107 -0.509817  ...  0.884141 -0.656548  3.106382 -0.126204   \n",
      "2  0.946321  1.030107 -0.509817  ...  0.884141 -0.656548  3.106382 -0.126204   \n",
      "3  0.945105  1.030176 -0.505772  ...  0.882250 -0.655979  3.104670 -0.125328   \n",
      "4  0.947631  1.035075 -0.506821  ...  0.885115 -0.656932  3.104566 -0.125624   \n",
      "\n",
      "         17        18        19  latitude   longitude  cluster_label  \n",
      "0 -1.456508  0.269211 -0.406401  33.62731  107.568449              2  \n",
      "1 -1.458185  0.267994 -0.399068  33.62731  107.568539              2  \n",
      "2 -1.458185  0.267994 -0.399068  33.62731  107.568629              2  \n",
      "3 -1.456884  0.268870 -0.398573  33.62731  107.568719              2  \n",
      "4 -1.458772  0.268010 -0.401849  33.62731  107.568809              2  \n",
      "\n",
      "[5 rows x 23 columns] \n",
      "\n",
      "=== Predicting Classes for DEC Data ===\n",
      "Generating predictions...\n",
      "\u001B[1m68936/68936\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 615us/step\n",
      "Saving predictions to:\n",
      "  C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\DEC_with_predictions.csv\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See that the predicted class and DEC cluster class C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\DEC_with_predictions.csv",
   "id": "64632a7404345621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:28:51.822079Z",
     "start_time": "2025-02-17T13:28:45.282997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score\n",
    ")\n",
    "\n",
    "# 1) Load the CSV containing DEC cluster labels + predicted_class\n",
    "file_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\transformer_output_latlong\\DEC_clustered\\DEC_with_predictions.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"=== DEC_with_predictions.csv loaded ===\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Sample rows:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "# 2) Build a contingency table (cross-tab) of cluster_label vs predicted_class\n",
    "#    This quickly shows how many points in each cluster ended up with each predicted_class.\n",
    "contingency = pd.crosstab(df['cluster_label'], df['predicted_class'])\n",
    "print(\"=== Contingency Table: cluster_label vs. predicted_class ===\")\n",
    "print(contingency)\n",
    "print()\n",
    "\n",
    "# 3) Convert predicted_class from strings to integer IDs for clustering metrics\n",
    "label_enc = LabelEncoder()\n",
    "y_true = label_enc.fit_transform(df['predicted_class'].values)  # \"True\" labels (supervised notion)\n",
    "y_cluster = df['cluster_label'].values  # Unsupervised cluster assignments\n",
    "\n",
    "# 4) Some popular cluster vs. label metrics:\n",
    "\n",
    "# (a) Confusion Matrix\n",
    "#    Note that typically confusion_matrix is for \"true_label\" vs \"predicted_label\".\n",
    "#    Here we treat \"predicted_class\" as 'true' and DEC cluster_label as 'predicted'.\n",
    "cm = confusion_matrix(y_true, y_cluster)\n",
    "print(\"=== Confusion Matrix (Predicted Class vs. Cluster Label) ===\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# (b) Adjusted Rand Index (ARI)\n",
    "#    Ranges from -1 to 1 (random to perfect clustering). 0 indicates random labeling.\n",
    "ari = adjusted_rand_score(y_true, y_cluster)\n",
    "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "\n",
    "# (c) Adjusted Mutual Information (AMI)\n",
    "#    Also 0 = random, 1 = perfect. Adjusted for chance.\n",
    "ami = adjusted_mutual_info_score(y_true, y_cluster)\n",
    "print(f\"Adjusted Mutual Info Score: {ami:.4f}\")\n",
    "\n",
    "# (d) Homogeneity & Completeness\n",
    "#    Homogeneity: 1 => each cluster contains only members of a single class.\n",
    "#    Completeness: 1 => each class is entirely contained within a single cluster.\n",
    "h = homogeneity_score(y_true, y_cluster)\n",
    "c = completeness_score(y_true, y_cluster)\n",
    "print(f\"Homogeneity Score: {h:.4f}\")\n",
    "print(f\"Completeness Score: {c:.4f}\")\n",
    "\n",
    "# (e) V-measure\n",
    "#    Harmonic mean of homogeneity & completeness. 1 => perfect.\n",
    "v = v_measure_score(y_true, y_cluster)\n",
    "print(f\"V-Measure: {v:.4f}\")\n",
    "\n",
    "print(\"\\n=== Analysis Complete ===\")\n"
   ],
   "id": "76fbb12d667c7e84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEC_with_predictions.csv loaded ===\n",
      "Columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', 'latitude', 'longitude', 'cluster_label', 'predicted_class']\n",
      "Sample rows:\n",
      "           0         1         2         3          4         5         6  \\\n",
      "0  0.155357  0.286328  0.393404  0.614512 -13.277342 -0.116932  0.244814   \n",
      "1  0.151448  0.289241  0.390746  0.617897 -13.285116 -0.115736  0.244864   \n",
      "2  0.151448  0.289241  0.390746  0.617897 -13.285116 -0.115736  0.244864   \n",
      "3  0.154539  0.290430  0.393342  0.620422 -13.284363 -0.124646  0.240445   \n",
      "4  0.153474  0.291959  0.391579  0.622427 -13.276670 -0.125364  0.236160   \n",
      "\n",
      "          7         8         9  ...        14        15        16        17  \\\n",
      "0  0.948458  1.034857 -0.511455  ... -0.656209  3.106227 -0.126195 -1.456508   \n",
      "1  0.946321  1.030107 -0.509817  ... -0.656548  3.106382 -0.126204 -1.458185   \n",
      "2  0.946321  1.030107 -0.509817  ... -0.656548  3.106382 -0.126204 -1.458185   \n",
      "3  0.945105  1.030176 -0.505772  ... -0.655979  3.104670 -0.125328 -1.456884   \n",
      "4  0.947631  1.035075 -0.506821  ... -0.656932  3.104566 -0.125624 -1.458772   \n",
      "\n",
      "         18        19  latitude   longitude  cluster_label  predicted_class  \n",
      "0  0.269211 -0.406401  33.62731  107.568449              2       deci_broad  \n",
      "1  0.267994 -0.399068  33.62731  107.568539              2       deci_broad  \n",
      "2  0.267994 -0.399068  33.62731  107.568629              2       deci_broad  \n",
      "3  0.268870 -0.398573  33.62731  107.568719              2       deci_broad  \n",
      "4  0.268010 -0.401849  33.62731  107.568809              2       deci_broad  \n",
      "\n",
      "[5 rows x 24 columns] \n",
      "\n",
      "=== Contingency Table: cluster_label vs. predicted_class ===\n",
      "predicted_class  deci_broad  ever_coni  larch_JP\n",
      "cluster_label                                   \n",
      "0                    157696     235776     78848\n",
      "1                     78848     157440     78848\n",
      "2                    473088     157696    157440\n",
      "3                    157696          0     78848\n",
      "4                     78592     236288     78848\n",
      "\n",
      "=== Confusion Matrix (Predicted Class vs. Cluster Label) ===\n",
      "[[157696  78848 473088 157696  78592]\n",
      " [235776 157440 157696      0 236288]\n",
      " [ 78848  78848 157440  78848  78848]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]]\n",
      "\n",
      "Adjusted Rand Index: 0.0815\n",
      "Adjusted Mutual Info Score: 0.0916\n",
      "Homogeneity Score: 0.1116\n",
      "Completeness Score: 0.0777\n",
      "V-Measure: 0.0916\n",
      "\n",
      "=== Analysis Complete ===\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In short, the DEC clusters do not map cleanly onto the (supervised) predicted classes—they are largely mixed. That’s not unexpected in an unsupervised approach; a single cluster may contain multiple species, and a single species may appear across several clusters. The low ARI and AMI confirm that there isn’t a strong one‐to‐one correspondence between the cluster labels and the final predicted classes.",
   "id": "f6d1182afe5c2544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Manual trend analysis",
   "id": "8b40420fadda2773"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:59:27.936489Z",
     "start_time": "2025-02-17T13:59:27.337884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# After training, predictions on the validation set:\n",
    "val_preds = model.predict(X_val)  # shape (val_samples, n_classes)\n",
    "val_pred_classes = np.argmax(val_preds, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_val, val_pred_classes)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, val_pred_classes))\n"
   ],
   "id": "182379790ef076f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m103/103\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\n",
      "Confusion Matrix:\n",
      " [[ 989    6    1    7    0]\n",
      " [   2  733    0   21    0]\n",
      " [   3    1  161   42    0]\n",
      " [  11   20   14 1102    0]\n",
      " [   0    1    0    2  177]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1003\n",
      "           1       0.96      0.97      0.97       756\n",
      "           2       0.91      0.78      0.84       207\n",
      "           3       0.94      0.96      0.95      1147\n",
      "           4       1.00      0.98      0.99       180\n",
      "\n",
      "    accuracy                           0.96      3293\n",
      "   macro avg       0.96      0.94      0.95      3293\n",
      "weighted avg       0.96      0.96      0.96      3293\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simple LSTM classifier",
   "id": "e8f76f94720cf3b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T14:22:46.245985Z",
     "start_time": "2025-02-17T14:21:08.444248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == 'cuda':\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name())\n",
    "\n",
    "##############################################################################\n",
    "# 1) Dataset Class for 12-Month Sequences\n",
    "##############################################################################\n",
    "class GroundtruthTimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads a CSV with repeated rows over months for each (lat, lon),\n",
    "    e.g. 12 rows for 12 months. We group them to form a single time-series\n",
    "    sample: shape (12, number_of_features). The 'Class' label is taken\n",
    "    from the first row's 'Class' (assuming each lat/lon is a single species).\n",
    "\n",
    "    If a lat/lon has fewer than 12 months, we can pad with the last row.\n",
    "    If it has more than 12, we can slice down to 12.\n",
    "\n",
    "    The user can define which columns are the 'feature_cols'.\n",
    "    We'll store 'label_encoder' for Class strings -> integers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, feature_cols=None, desired_length=12):\n",
    "        print(f\"Reading CSV file: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path, parse_dates=['image_date'])\n",
    "\n",
    "        if feature_cols is None:\n",
    "            # You indicated at least 20 features: AOT..WVP, EVI, NDVI, NDWI, etc.\n",
    "            # Adjust to exactly the ones you want. Example below:\n",
    "            self.feature_cols = [\n",
    "                \"AOT\",\"B1\",\"B11\",\"B12\",\"B2\",\"B3\",\"B4\",\"B5\",\n",
    "                \"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"EVI\",\"NDVI\",\"NDWI\",\n",
    "                \"TCI_B\",\"TCI_G\",\"TCI_R\",\"WVP\"\n",
    "            ]\n",
    "        else:\n",
    "            self.feature_cols = feature_cols\n",
    "\n",
    "        # We'll group by (latitude, longitude). If you prefer rounding, do so here.\n",
    "        # e.g. df['lat_lon_id'] = ...\n",
    "        # But let's assume the lat/lon values are consistent for each patch.\n",
    "        df[\"lat_lon_id\"] = df[\"latitude\"].astype(str) + \"_\" + df[\"longitude\"].astype(str)\n",
    "\n",
    "        # Encode \"Class\" if present\n",
    "        if \"Class\" not in df.columns:\n",
    "            raise ValueError(\"CSV must have 'Class' column for supervised training.\")\n",
    "\n",
    "        # Build label encoder\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        class_strs = df[\"Class\"].unique()\n",
    "        self.label_encoder.fit(class_strs)\n",
    "\n",
    "        self.desired_length = desired_length\n",
    "        self.samples = []  # each is a (time_series_tensor, label_int)\n",
    "\n",
    "        # group by lat_lon_id\n",
    "        grouped = df.groupby(\"lat_lon_id\", sort=False)\n",
    "        for group_id, group_data in grouped:\n",
    "            group_data = group_data.sort_values(\"image_date\")\n",
    "            arr = group_data[self.feature_cols].to_numpy(dtype=float)  # shape (T, D)\n",
    "            T, D = arr.shape\n",
    "\n",
    "            if T < desired_length:\n",
    "                # pad with last row\n",
    "                pad = np.tile(arr[-1:], (desired_length - T, 1))\n",
    "                arr = np.vstack([arr, pad])\n",
    "            elif T > desired_length:\n",
    "                arr = arr[:desired_length]\n",
    "\n",
    "            # Convert to torch tensor\n",
    "            seq_tensor = torch.tensor(arr, dtype=torch.float)  # shape (12, #features)\n",
    "            # Label: from the first row's 'Class'\n",
    "            class_str = group_data.iloc[0][\"Class\"]\n",
    "            label_id = self.label_encoder.transform([class_str])[0]\n",
    "\n",
    "            self.samples.append((seq_tensor, label_id))\n",
    "\n",
    "        print(f\"Constructed {len(self.samples)} time-series samples from {csv_path}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, label = self.samples[idx]\n",
    "        return seq, label\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) Neural Network Model: LSTM (or GRU) to handle 12×D sequences\n",
    "##############################################################################\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM-based classifier that:\n",
    "      - Reads sequences of shape (T=12, D=20+).\n",
    "      - Optionally: you can add multiple LSTM layers, dropout, etc.\n",
    "      - Then we take the final hidden state or last time step for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # You can also use nn.GRU(...) if you prefer a GRU\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=drop_prob,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        # final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: shape (B, T, D)\n",
    "        # Initialize hidden + cell states:\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM forward\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # out shape: (B, T, hidden_dim)\n",
    "        # hn shape: (num_layers, B, hidden_dim) => final hidden state for last time step\n",
    "\n",
    "        # We'll take the last time step's output\n",
    "        # or we can just use hn[-1], which is the last layer's hidden state\n",
    "        last_out = out[:, -1, :]  # shape (B, hidden_dim)\n",
    "        logits = self.fc(last_out)  # shape (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 3) Main Training Pipeline\n",
    "##############################################################################\n",
    "def train_lstm_classifier(\n",
    "    csv_path,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    drop_prob=0.2,\n",
    "    save_path=\"lstm_classifier.pth\"\n",
    "):\n",
    "    # 1) Create Dataset\n",
    "    dataset = GroundtruthTimeSeriesDataset(csv_path)\n",
    "\n",
    "    # 2) Train/Val Split\n",
    "    ds_len = len(dataset)\n",
    "    train_size = int(0.8 * ds_len)\n",
    "    val_size = ds_len - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_classes = len(dataset.label_encoder.classes_)\n",
    "    input_dim = len(dataset.feature_cols)\n",
    "\n",
    "    print(f\"Dataset has {ds_len} time-series samples, {num_classes} distinct classes.\")\n",
    "    print(f\"Using LSTM with input_dim={input_dim}, hidden_dim={hidden_dim}, num_classes={num_classes}.\")\n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "\n",
    "    # 3) Build Model\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=num_classes,\n",
    "        drop_prob=drop_prob\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 4) Training Loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for X, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} (train)\", leave=False):\n",
    "            X = X.to(device)            # shape (B, 12, input_dim)\n",
    "            y = y.to(device)            # shape (B,)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)           # shape (B, num_classes)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += X.size(0)\n",
    "        train_acc = correct / total\n",
    "        avg_loss = total_loss / total\n",
    "\n",
    "        # ---- validate ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct, val_total = 0, 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for Xv, yv in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} (val)\", leave=False):\n",
    "                Xv = Xv.to(device)\n",
    "                yv = yv.to(device)\n",
    "                logits_v = model(Xv)\n",
    "                loss_v = criterion(logits_v, yv)\n",
    "                val_loss += loss_v.item() * Xv.size(0)\n",
    "                preds_v = torch.argmax(logits_v, dim=1)\n",
    "                val_correct += (preds_v == yv).sum().item()\n",
    "                val_total += Xv.size(0)\n",
    "                all_val_preds.extend(preds_v.cpu().numpy())\n",
    "                all_val_labels.extend(yv.cpu().numpy())\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        print(f\"[Epoch {epoch}/{epochs}] Train Loss={avg_loss:.4f}, Train Acc={train_acc:.2f}, \"\n",
    "              f\"Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.2f}\")\n",
    "\n",
    "    # 5) Save the model\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"\\nModel saved to {save_path}.\")\n",
    "\n",
    "    # 6) Final Confusion Matrix on val set\n",
    "    cm = confusion_matrix(all_val_labels, all_val_preds)\n",
    "    print(\"\\nConfusion Matrix (on validation set):\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Convert numeric labels back to strings\n",
    "    inv_class_map = {i: c for i, c in enumerate(dataset.label_encoder.classes_)}\n",
    "    all_val_preds_str = [inv_class_map[p] for p in all_val_preds]\n",
    "    all_val_labels_str = [inv_class_map[l] for l in all_val_labels]\n",
    "    print(classification_report(all_val_labels_str, all_val_preds_str))\n",
    "\n",
    "    return model, dataset\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 4) Example Usage\n",
    "##############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    groundtruth_csv = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\groundtruth_cleaned_final.csv\"\n",
    "    model, ds = train_lstm_classifier(\n",
    "        csv_path=groundtruth_csv,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        drop_prob=0.2,\n",
    "        save_path=\"lstm_classifier_model.pth\"\n",
    "    )\n"
   ],
   "id": "c0bd5f75d7356ba4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA RTX 2000 Ada Generation\n",
      "Reading CSV file: C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\groundtruth_cleaned_final.csv\n",
      "Constructed 16464 time-series samples from C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\groundtruth_raw\\time_series_cleaned\\groundtruth_cleaned_final.csv.\n",
      "Dataset has 16464 time-series samples, 5 distinct classes.\n",
      "Using LSTM with input_dim=20, hidden_dim=128, num_classes=5.\n",
      "Training for 100 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100] Train Loss=0.4724, Train Acc=0.83, Val Loss=0.2206, Val Acc=0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/100] Train Loss=0.3468, Train Acc=0.88, Val Loss=0.2372, Val Acc=0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/100] Train Loss=0.2778, Train Acc=0.90, Val Loss=0.1838, Val Acc=0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/100] Train Loss=0.2478, Train Acc=0.91, Val Loss=0.1461, Val Acc=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/100] Train Loss=0.2362, Train Acc=0.92, Val Loss=0.1371, Val Acc=0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/100] Train Loss=0.1798, Train Acc=0.94, Val Loss=0.1140, Val Acc=0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/100] Train Loss=0.1867, Train Acc=0.93, Val Loss=0.1258, Val Acc=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/100] Train Loss=0.1880, Train Acc=0.93, Val Loss=0.1003, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/100] Train Loss=0.1691, Train Acc=0.94, Val Loss=0.0983, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] Train Loss=0.1589, Train Acc=0.94, Val Loss=0.0925, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/100] Train Loss=0.1480, Train Acc=0.95, Val Loss=0.0820, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/100] Train Loss=0.1719, Train Acc=0.94, Val Loss=0.1572, Val Acc=0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/100] Train Loss=0.1534, Train Acc=0.94, Val Loss=0.0835, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/100] Train Loss=0.1475, Train Acc=0.95, Val Loss=0.0957, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/100] Train Loss=0.1553, Train Acc=0.94, Val Loss=0.0782, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/100] Train Loss=0.1424, Train Acc=0.95, Val Loss=0.0782, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/100] Train Loss=0.1159, Train Acc=0.96, Val Loss=0.0876, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/100] Train Loss=0.1364, Train Acc=0.95, Val Loss=0.0751, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/100] Train Loss=0.1204, Train Acc=0.96, Val Loss=0.0696, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/100] Train Loss=0.1315, Train Acc=0.95, Val Loss=0.1193, Val Acc=0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/100] Train Loss=0.1486, Train Acc=0.95, Val Loss=0.0800, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/100] Train Loss=0.1339, Train Acc=0.95, Val Loss=0.0997, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/100] Train Loss=0.1141, Train Acc=0.96, Val Loss=0.0677, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/100] Train Loss=0.1135, Train Acc=0.96, Val Loss=0.0753, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/100] Train Loss=0.1235, Train Acc=0.96, Val Loss=0.0697, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/100] Train Loss=0.1284, Train Acc=0.95, Val Loss=0.0779, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27/100] Train Loss=0.1074, Train Acc=0.96, Val Loss=0.0616, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28/100] Train Loss=0.1081, Train Acc=0.96, Val Loss=0.0693, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29/100] Train Loss=0.1006, Train Acc=0.96, Val Loss=0.0670, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/100] Train Loss=0.1169, Train Acc=0.96, Val Loss=0.0763, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31/100] Train Loss=0.1415, Train Acc=0.95, Val Loss=0.0761, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32/100] Train Loss=0.1266, Train Acc=0.95, Val Loss=0.0686, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33/100] Train Loss=0.1097, Train Acc=0.96, Val Loss=0.0710, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34/100] Train Loss=0.0985, Train Acc=0.97, Val Loss=0.0707, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/100] Train Loss=0.1040, Train Acc=0.96, Val Loss=0.0867, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36/100] Train Loss=0.1094, Train Acc=0.96, Val Loss=0.0695, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37/100] Train Loss=0.1192, Train Acc=0.96, Val Loss=0.0677, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38/100] Train Loss=0.1122, Train Acc=0.96, Val Loss=0.0581, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39/100] Train Loss=0.1112, Train Acc=0.96, Val Loss=0.0644, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40/100] Train Loss=0.1023, Train Acc=0.96, Val Loss=0.0790, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41/100] Train Loss=0.1063, Train Acc=0.96, Val Loss=0.0672, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42/100] Train Loss=0.1142, Train Acc=0.96, Val Loss=0.0609, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43/100] Train Loss=0.1023, Train Acc=0.96, Val Loss=0.0742, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44/100] Train Loss=0.1192, Train Acc=0.96, Val Loss=0.0750, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45/100] Train Loss=0.1229, Train Acc=0.95, Val Loss=0.0571, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46/100] Train Loss=0.1063, Train Acc=0.96, Val Loss=0.0742, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/100] Train Loss=0.1035, Train Acc=0.96, Val Loss=0.0658, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/100] Train Loss=0.0951, Train Acc=0.96, Val Loss=0.0601, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49/100] Train Loss=0.1231, Train Acc=0.96, Val Loss=0.0744, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 50/100] Train Loss=0.1051, Train Acc=0.96, Val Loss=0.0629, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 51/100] Train Loss=0.1009, Train Acc=0.97, Val Loss=0.0663, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 52/100] Train Loss=0.1201, Train Acc=0.96, Val Loss=0.0735, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 53/100] Train Loss=0.1124, Train Acc=0.96, Val Loss=0.0598, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 54/100] Train Loss=0.1112, Train Acc=0.96, Val Loss=0.0528, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 55/100] Train Loss=0.1083, Train Acc=0.96, Val Loss=0.0626, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 56/100] Train Loss=0.0866, Train Acc=0.97, Val Loss=0.0681, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 57/100] Train Loss=0.1007, Train Acc=0.96, Val Loss=0.0616, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 58/100] Train Loss=0.1076, Train Acc=0.96, Val Loss=0.0649, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 59/100] Train Loss=0.1057, Train Acc=0.96, Val Loss=0.0599, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 60/100] Train Loss=0.0896, Train Acc=0.97, Val Loss=0.0606, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 61/100] Train Loss=0.0979, Train Acc=0.96, Val Loss=0.0566, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 62/100] Train Loss=0.0882, Train Acc=0.97, Val Loss=0.0496, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 63/100] Train Loss=0.0879, Train Acc=0.97, Val Loss=0.0556, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64/100] Train Loss=0.0852, Train Acc=0.97, Val Loss=0.0758, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65/100] Train Loss=0.0979, Train Acc=0.96, Val Loss=0.0571, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 66/100] Train Loss=0.1094, Train Acc=0.96, Val Loss=0.0660, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 67/100] Train Loss=0.0949, Train Acc=0.96, Val Loss=0.0522, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 68/100] Train Loss=0.0891, Train Acc=0.97, Val Loss=0.0490, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 69/100] Train Loss=0.1019, Train Acc=0.96, Val Loss=0.0518, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 70/100] Train Loss=0.0917, Train Acc=0.97, Val Loss=0.0498, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 71/100] Train Loss=0.1010, Train Acc=0.96, Val Loss=0.0581, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 72/100] Train Loss=0.0921, Train Acc=0.97, Val Loss=0.0627, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 73/100] Train Loss=0.0895, Train Acc=0.97, Val Loss=0.0517, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 74/100] Train Loss=0.0864, Train Acc=0.97, Val Loss=0.0524, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 75/100] Train Loss=0.1083, Train Acc=0.96, Val Loss=0.0679, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 76/100] Train Loss=0.0860, Train Acc=0.97, Val Loss=0.0540, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 77/100] Train Loss=0.0798, Train Acc=0.97, Val Loss=0.0508, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 78/100] Train Loss=0.0707, Train Acc=0.97, Val Loss=0.0511, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 79/100] Train Loss=0.0722, Train Acc=0.97, Val Loss=0.0509, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80/100] Train Loss=0.0713, Train Acc=0.97, Val Loss=0.0573, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 81/100] Train Loss=0.0898, Train Acc=0.97, Val Loss=0.0501, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 82/100] Train Loss=0.0779, Train Acc=0.97, Val Loss=0.0585, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 83/100] Train Loss=0.1023, Train Acc=0.96, Val Loss=0.0765, Val Acc=0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 84/100] Train Loss=0.0868, Train Acc=0.97, Val Loss=0.0572, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 85/100] Train Loss=0.0791, Train Acc=0.97, Val Loss=0.0700, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 86/100] Train Loss=0.0840, Train Acc=0.97, Val Loss=0.0633, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 87/100] Train Loss=0.0960, Train Acc=0.97, Val Loss=0.0443, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 88/100] Train Loss=0.0950, Train Acc=0.97, Val Loss=0.0518, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 89/100] Train Loss=0.0780, Train Acc=0.97, Val Loss=0.0500, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 90/100] Train Loss=0.0765, Train Acc=0.97, Val Loss=0.0510, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 91/100] Train Loss=0.0853, Train Acc=0.97, Val Loss=0.0472, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 92/100] Train Loss=0.0939, Train Acc=0.96, Val Loss=0.0570, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 93/100] Train Loss=0.1036, Train Acc=0.96, Val Loss=0.0474, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 94/100] Train Loss=0.0915, Train Acc=0.97, Val Loss=0.0578, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 95/100] Train Loss=0.0808, Train Acc=0.97, Val Loss=0.0587, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96/100] Train Loss=0.0831, Train Acc=0.97, Val Loss=0.0635, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 97/100] Train Loss=0.0997, Train Acc=0.96, Val Loss=0.0600, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 98/100] Train Loss=0.0875, Train Acc=0.97, Val Loss=0.0517, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 99/100] Train Loss=0.0788, Train Acc=0.97, Val Loss=0.0570, Val Acc=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 100/100] Train Loss=0.0835, Train Acc=0.97, Val Loss=0.0514, Val Acc=0.98\n",
      "\n",
      "Model saved to lstm_classifier_model.pth.\n",
      "\n",
      "Confusion Matrix (on validation set):\n",
      "[[ 965    0    0    0    0]\n",
      " [   5  791    0   10    2]\n",
      " [   0    0  172   20    0]\n",
      " [   4    3   13 1139    2]\n",
      " [   0    1    0    0  166]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  deci_broad       0.99      1.00      1.00       965\n",
      "   ever_coni       0.99      0.98      0.99       808\n",
      "    larch_CN       0.93      0.90      0.91       192\n",
      "    larch_JP       0.97      0.98      0.98      1161\n",
      "   shrubland       0.98      0.99      0.99       167\n",
      "\n",
      "    accuracy                           0.98      3293\n",
      "   macro avg       0.97      0.97      0.97      3293\n",
      "weighted avg       0.98      0.98      0.98      3293\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[[ 965    0    0    0    0]   -> deci_broad\n",
    " [   5  791    0   10    2]   -> ever_coni\n",
    " [   0    0  172   20    0]   -> larch_CN\n",
    " [   4    3   13 1139    2]   -> larch_JP\n",
    " [   0    1    0    0  166]]  -> shrubland\n",
    "\n",
    "Class 0 (deci_broad) has 965 samples, all but 5 are correct → near 100% precision/recall.\n",
    "Class 1 (ever_coni) has 808 samples: 791 correct → ~98% accuracy for that class.\n",
    "Class 2 (larch_CN) has 192 samples: 172 correct, 20 predicted as larch_JP. ~90% recall, some confusion with class 3.\n",
    "Class 3 (larch_JP) has 1161 samples, 1139 correct. Also near 98% recall.\n",
    "Class 4 (shrubland) with 167 samples: 166 correct → 99% recall."
   ],
   "id": "37d5c0476682139d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing the LSTM model on the unseen data.",
   "id": "41fc5bc5a889a99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0. Check CUDA / GPU availability\n",
    "# ---------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"=== Checking for GPU (CUDA) availability ===\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU is available! Using device: {device}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. LSTM Model Architecture\n",
    "# ---------------------------------------------------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=drop_prob,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Initialize hidden/cell\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # last time-step output\n",
    "        last_out = out[:, -1, :]  # shape (B, hidden_dim)\n",
    "        logits = self.fc(last_out)\n",
    "        return logits\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. BlindPatchDataset for unlabeled CSV\n",
    "# ---------------------------------------------------------------------\n",
    "class BlindPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For a single CSV with columns:\n",
    "      [AOT,B1,B11,B12,B2,B3,B4,B5,B6,B7,B8,B8A,B9,EVI,NDVI,NDWI,\n",
    "       TCI_B,TCI_G,TCI_R,WVP, image_date,longitude,latitude]\n",
    "    We group by lat/lon, sort by image_date, ensure T=12 (pad or truncate).\n",
    "    Returns (seq_tensor, (lat, lon)) for each pixel/time-series.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, feature_cols=None, desired_length=12):\n",
    "        print(f\"\\n--- Reading CSV file: {csv_path}\")\n",
    "        self.df = pd.read_csv(csv_path, parse_dates=[\"image_date\"])\n",
    "        print(f\"[DEBUG] Shape of read DataFrame: {self.df.shape}\")\n",
    "\n",
    "        if feature_cols is None:\n",
    "            self.feature_cols = [\n",
    "                \"AOT\",\"B1\",\"B11\",\"B12\",\"B2\",\"B3\",\"B4\",\"B5\",\n",
    "                \"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"EVI\",\"NDVI\",\"NDWI\",\n",
    "                \"TCI_B\",\"TCI_G\",\"TCI_R\",\"WVP\"\n",
    "            ]\n",
    "        else:\n",
    "            self.feature_cols = feature_cols\n",
    "\n",
    "        # Combine lat/lon into a group id\n",
    "        self.df[\"lat_lon_id\"] = (\n",
    "            self.df[\"latitude\"].round(6).astype(str)\n",
    "            + \"_\"\n",
    "            + self.df[\"longitude\"].round(6).astype(str)\n",
    "        )\n",
    "\n",
    "        self.desired_length = desired_length\n",
    "        self.samples = []\n",
    "        self.latlons = []\n",
    "\n",
    "        grouped = self.df.groupby(\"lat_lon_id\", sort=False)\n",
    "        for group_id, gdata in grouped:\n",
    "            gdata_sorted = gdata.sort_values(\"image_date\")\n",
    "            arr = gdata_sorted[self.feature_cols].to_numpy(dtype=float)  # shape (T, D)\n",
    "            T, D = arr.shape\n",
    "\n",
    "            if T < desired_length:\n",
    "                # pad with last row\n",
    "                pad = np.tile(arr[-1:], (desired_length - T, 1))\n",
    "                arr = np.vstack([arr, pad])\n",
    "            elif T > desired_length:\n",
    "                arr = arr[:desired_length]\n",
    "\n",
    "            seq_tensor = torch.tensor(arr, dtype=torch.float)\n",
    "            lat0 = gdata_sorted.iloc[0][\"latitude\"]\n",
    "            lon0 = gdata_sorted.iloc[0][\"longitude\"]\n",
    "            self.samples.append(seq_tensor)\n",
    "            self.latlons.append((lat0, lon0))\n",
    "\n",
    "        print(f\"[DEBUG] Constructed {len(self.samples)} time-series samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.latlons[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.5 Custom Collate Function\n",
    "# ---------------------------------------------------------------------\n",
    "def collate_blind_patches(batch):\n",
    "    \"\"\"\n",
    "    Expects a list of (seq_tensor, (lat, lon)) pairs.\n",
    "    We'll stack all seq_tensors into (B, T, D),\n",
    "    and keep coords as a list of (lat, lon).\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    coords = []\n",
    "    for (seq_tensor, (lat, lon)) in batch:\n",
    "        seqs.append(seq_tensor)\n",
    "        coords.append((lat, lon))\n",
    "\n",
    "    # Stack the sequence tensors along the batch dimension\n",
    "    seq_batch = torch.stack(seqs, dim=0)  # shape (B, 12, feature_dim)\n",
    "\n",
    "    return seq_batch, coords\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Inference Function\n",
    "# ---------------------------------------------------------------------\n",
    "def infer_on_blind_data(\n",
    "    folder_path,                   # path to raw_cleaned folder\n",
    "    model_path=\"lstm_classifier_model.pth\",\n",
    "    output_folder=\"inference_results\",\n",
    "    batch_size=32,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    drop_prob=0.2,\n",
    "    feature_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    - folder_path: Directory that contains *cleaned.csv files\n",
    "    - model_path: Path to .pth model weights\n",
    "    - output_folder: Where we store the results\n",
    "    - batch_size, hidden_dim, num_layers, drop_prob: must match your training config\n",
    "    - feature_cols: same features used in training\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Inference on Blind Data ===\")\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*cleaned.csv\"))\n",
    "    print(f\"Found {len(csv_files)} files in {folder_path} that match '*cleaned.csv'\")\n",
    "\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    print(\"Feature cols:\", feature_cols)\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    # Suppose you have 5 classes from training\n",
    "    num_classes = 5\n",
    "    input_dim = len(feature_cols) if feature_cols else 20\n",
    "\n",
    "    # Rebuild the same LSTM\n",
    "    print(f\"[INFO] Building LSTM model: input_dim={input_dim}, hidden_dim={hidden_dim}, num_classes={num_classes}\")\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=num_classes,\n",
    "        drop_prob=drop_prob\n",
    "    )\n",
    "    print(f\"[INFO] Loading state_dict from: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Label mapping from training\n",
    "    label_mapping = [\"deci_broad\", \"ever_coni\", \"larch_CN\", \"larch_JP\", \"shrubland\"]\n",
    "\n",
    "    # Ensure output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for csv_path in tqdm(csv_files, desc=\"CSV Files\", unit=\"file\"):\n",
    "        print(f\"\\n--- Inference on file: {csv_path}\")\n",
    "        try:\n",
    "            ds = BlindPatchDataset(csv_path, feature_cols=feature_cols)\n",
    "            # Pass our custom collate function:\n",
    "            dl = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_blind_patches)\n",
    "\n",
    "            print(f\"[DEBUG] Dataset has {len(ds)} samples. Beginning inference...\")\n",
    "\n",
    "            all_preds = []\n",
    "            all_coords = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for seq_batch, coords_batch in tqdm(dl, desc=\"Predicting Batches\", leave=False):\n",
    "                    # seq_batch: shape (B, 12, input_dim)\n",
    "                    # coords_batch: list of length B, each is (lat, lon)\n",
    "                    seq_batch = seq_batch.to(device)\n",
    "                    logits = model(seq_batch)\n",
    "                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                    all_preds.extend(preds)\n",
    "                    all_coords.extend(coords_batch)\n",
    "\n",
    "            if len(all_preds) != len(all_coords):\n",
    "                raise ValueError(f\"[ERROR] length mismatch: preds={len(all_preds)}, coords={len(all_coords)}\")\n",
    "\n",
    "            pred_classes = [label_mapping[p] for p in all_preds]\n",
    "\n",
    "            # Build results DataFrame\n",
    "            print(f\"[DEBUG] Building DataFrame for {len(pred_classes)} predictions...\")\n",
    "            results_df = pd.DataFrame({\n",
    "                \"latitude\": [c[0] for c in all_coords],\n",
    "                \"longitude\": [c[1] for c in all_coords],\n",
    "                \"predicted_class\": pred_classes\n",
    "            })\n",
    "            print(results_df.head(5))\n",
    "\n",
    "            base_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "            out_csv = os.path.join(output_folder, base_name + \"_with_predictions.csv\")\n",
    "            results_df.to_csv(out_csv, index=False)\n",
    "            print(f\"Results saved to: {out_csv}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed on file {csv_path}\\nReason: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Example usage\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"C:\\Users\\jmm267\\Downloads\\Binbin\\Dataset\\raw_cleaned\"\n",
    "    features_used = [\n",
    "        \"AOT\",\"B1\",\"B11\",\"B12\",\"B2\",\"B3\",\"B4\",\"B5\",\n",
    "        \"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"EVI\",\"NDVI\",\"NDWI\",\n",
    "        \"TCI_B\",\"TCI_G\",\"TCI_R\",\"WVP\"\n",
    "    ]\n",
    "\n",
    "    infer_on_blind_data(\n",
    "        folder_path=folder_path,\n",
    "        model_path=\"lstm_classifier_model.pth\",\n",
    "        output_folder=\"larchCN_inference_results\",\n",
    "        batch_size=32,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        drop_prob=0.2,\n",
    "        feature_cols=features_used\n",
    "    )\n"
   ],
   "id": "ad310d9fc69967fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
